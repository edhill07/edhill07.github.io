<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ed Hill - Bank of England</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.6;
            color: #333;
            background-color: #fafafa;
        }

        .banner {
            width: 100%;
            height: 300px;
            background: url('images/IMG_5667.jpg') center center/cover no-repeat;
            display: flex;
            align-items: center;
            justify-content: center;
            color: black;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        /* .banner::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1000 300"><defs><pattern id="grid" width="40" height="40" patternUnits="userSpaceOnUse"><path d="M 40 0 L 0 0 0 40" fill="none" stroke="rgba(255,255,255,0.1)" stroke-width="1"/></pattern></defs><rect width="100%" height="100%" fill="url(%23grid)"/></svg>');
            opacity: 0.9;
        } */

        .banner-content {
            z-index: 1;
        }

        .intro h1 {
            font-size: 3.5em;
            font-weight: 300;
            margin-bottom: 0.5em;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .banner p {
            font-size: 1.3em;
            font-weight: 300;
            opacity: 0.9;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        .content {
            background: white;
            margin: -50px auto 0;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            position: relative;
            z-index: 2;
            overflow: hidden;
        }

        .intro {
            padding: 60px 40px 40px;
            border-bottom: 1px solid #eee;
            text-align: center;
        }

        .intro p {
            font-size: 1.2em;
            color: #666;
            margin-bottom: 1em;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }

        .section {
            padding: 50px 40px;
        }

        .section:not(:last-child) {
            border-bottom: 1px solid #eee;
        }

        .section-title {
            font-size: 2.2em;
            color: #2c3e50;
            margin-bottom: 2rem;
            text-align: center;
            position: relative;
        }

        .section-title::after {
            content: '';
            display: block;
            width: 80px;
            height: 3px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            margin: 15px auto;
        }

        .subsection-title {
            font-size: 1.6em;
            color: #34495e;
            margin: 2.5rem 0 1.5rem;
            border-left: 4px solid #667eea;
            padding-left: 20px;
        }

        .project {
            margin-bottom: 2.5rem;
            padding: 2rem;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 5px solid #667eea;
        }

        .project-title {
            font-size: 1.3em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 0.5rem;
        }

        .project-collaborators {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 1rem;
        }

        .project-highlight {
            background: #e8f4f8;
            padding: 1.2rem;
            border-radius: 8px;
            margin: 1rem 0;
            border-left: 4px solid #3498db;
            font-style: italic;
            font-weight: 500;
        }

        .project-description {
            text-align: justify;
            line-height: 1.7;
        }

        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 2rem;
            margin-top: 2rem;
        }

        .paper-card {
            background: white;
            border-radius: 12px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
            overflow: hidden;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 35px rgba(0,0,0,0.15);
        }

        .paper-image {
            width: 100%;
            height: 200px;
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 1.2em;
            font-weight: 500;
            text-align: center;
            padding: 20px;
        }

        .paper-content {
            padding: 1.5rem;
        }

        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 0.5rem;
        }

        .paper-title a {
            color: #2c3e50;
            text-decoration: none;
        }

        .paper-title a:hover {
            color: #667eea;
            text-decoration: underline;
        }

        .paper-authors {
            color: #7f8c8d;
            font-style: italic;
            margin-bottom: 1rem;
            font-size: 0.95em;
        }

        .paper-abstract {
            color: #555;
            line-height: 1.6;
            text-align: justify;
        }

        .footer-section {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 40px;
        }

        .footer-section a {
            color: #67eea;
            text-decoration: none;
        }

        .footer-section a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .banner h1 {
                font-size: 2.5em;
            }
            
            .banner p {
                font-size: 1.1em;
            }
            
            .section {
                padding: 30px 20px;
            }
            
            .intro {
                padding: 40px 20px 30px;
            }
            
            .papers-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header class="banner">
        <!-- <div class="banner-content">
            <h1>Ed Hill</h1>
            <p>Senior Research Data Scientist | Bank of England</p>
        </div> -->
    </header>

    <div class="container">
        <div class="content">
            <section class="intro">
                <h1>Ed Hill</h1>
              <p>Senior Research Data Scientist | Advanced Analytics | Bank of England</p>
                <!-- <p>I work in the Advanced Analytics (AA) division at the Bank of England.</p> -->
                </section>

            <section class="section">
                <h2 class="section-title">Recent Papers</h2>
                <p style="text-align: center;" >See <a href="https://scholar.google.com/citations?hl=en&user=mutaoa0AAAAJ">Google Scholar</a> for a full list of publications.</p>
                
                <div class="papers-grid">

                    <div class="paper-card">
                        <div class="paper-image" style="background: url('images/GPT_IA.JPG') center center/cover no-repeat;">
                        </div>
                        <div class="paper-content">
                            <div class="paper-title">
                                <a href="https://arxiv.org/abs/2512.14306">Inflation Attitudes of Large Language Models</a>
                            </div>
                            <div class="paper-authors">with Niki Anesti and Andi Joseph, 2025, arXiv</div>
                            <div class="paper-abstract">
                                This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, with our quasi-experimental design exploiting the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content.
                            </div>
                        </div>
                    </div>
                    
                    <div class="paper-card">
                        <div class="paper-image" style="background: url('images/bu_ukjpn.JPG') center center/cover no-repeat;">
                        </div>
                        <div class="paper-content">
                            <div class="paper-title">
                                <a href="https://bankunderground.co.uk/2025/11/13/all-shocks-are-different-insights-from-sentiment-and-topic-analysis-using-llms">All shocks are different: insights from sentiment and topic analysis using LLMs</a>
                            </div>
                            <div class="paper-authors">with Iulia Bucur, 2025, Bank Underground blog post</div>
                            <div class="paper-abstract">
                                Modern language models are powerful tools: but how can we use them in economic policymaking? We propose decomposing the metrics which Large Language Models (LLMs) can derive from text data to offer insights from large collections of documents in a highly interpretable format. This approach aims to bridge the gap between natural language processing (NLP) techniques and economic decision-making, offering a richer, more context-aware understanding of complex economic phenomena.
                            </div>
                        </div>
                    </div>
                    
                    <div class="paper-card">
                        <div class="paper-image" style="background: url('images/economic_facts.JPG') center center/cover no-repeat;">
                        </div>
                        <div class="paper-content">
                            <div class="paper-title">
                                <a href="https://arxiv.org/pdf/2505.08662">Revealing economic facts: LLMs know more than they say</a>
                            </div>
                            <div class="paper-authors">with Marcus Buckmann and Quynh Anh Nguyen, 2025, arXiv</div>
                            <div class="paper-abstract">
                                We investigate whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, we show that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly.
                            </div>
                        </div>
                    </div>

                    <div class="paper-card">
                        <div class="paper-image" style="background: url('images/PLR.JPG') center center/cover no-repeat;">
                        </div>
                        <div class="paper-content">
                            <div class="paper-title">
                                <a href="https://www.bankofengland.co.uk/-/media/boe/files/working-paper/2025/improving-text-classification-logistic-regression-llms-tens-of-shot-classifiers.pdf">Logistic Regression makes small LLMs strong and explainable "tens-of-shot" classifiers</a>
                            </div>
                            <div class="paper-authors">with Marcus Buckmann, 2025, Bank of England Staff Working Paper</div>
                            <div class="paper-abstract">
                                For simple classification tasks, we show that users can benefit from the advantages of using small, local, generative language models instead of large commercial models without a trade-off in performance or introducing extra labelling costs. Through experiments on 17 sentence classification tasks (2-4 classes), we show that penalised logistic regression on the embeddings from a small LLM equals (and usually betters) the performance of a large LLM in the "tens-of-shot" regime.
                            </div>
                        </div>
                    </div>

                    <div class="paper-card">
                        <div class="paper-image" style="background: url('images/Schrodinger.JPG') center center/cover no-repeat;">
                        </div>
                        <div class="paper-content">
                            <div class="paper-title">
                                <a href="https://bankunderground.co.uk/2024/01/18/schrodingers-market-what-the-quantum-internet-could-mean-for-the-financial-system/">Schrodinger's market: what the quantum internet could mean for the financial system</a>
                            </div>
                            <div class="paper-authors">2024, Bank Underground blog post</div>
                            <div class="paper-abstract">
                                Once the stuff of science fiction, quantum technologies are advancing fast. Individual quantum computers are finding a range of applications, primarily driven by the immense speed-ups they offer over normal computers. This blog post starts to think about what this new interconnected quantum world means for the financial system. What could the first 'quantum markets' look like?
                            </div>
                        </div>
                    </div>

                    <div class="paper-card">
                        <div class="paper-image" style="background: url('images/RL.JPG') center center/cover no-repeat;">
                        </div>
                        <div class="paper-content">
                            <div class="paper-title">
                                <a href="https://arxiv.org/pdf/2103.16977">Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning</a>
                            </div>
                            <div class="paper-authors">with Marco Bardoscia and Arthur Turrell, 2021, arXiv</div>
                            <div class="paper-abstract">
                                General equilibrium macroeconomic models are a core tool used by policymakers to understand a nation's economy. We use techniques from reinforcement learning to solve such models incorporating heterogeneous agents in a way that is simple, extensible, and computationally efficient. The COVID-19 pandemic has highlighted the importance of heterogeneity in macroeconomic outcomes and the need for models that can more easily incorporate it.
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="section">
              <!-- <p>My role mixes research and more operational work, the two generally overlapping within each project, and I technically manage a couple of AA colleagues.</p> -->
            
                <h2 class="section-title">Current Projects</h2>

                <h3 class="subsection-title">Language Models</h3>

                <div class="project">
                    <div class="project-title">Evaluating RAG</div>
                    <div class="project-collaborators">with Tunrayo AdelekeLarodo, Matthew Leong and Supachai Saengthong</div>
                    <div class="project-highlight">
                        LLMs are great for researching and finding information – looking at documents directly, or quickly summing up tens of thousands of words of search results (this is "RAG") – but how do we know they're doing it well? Are their answers accurate? Have they captured everything?
                    </div>
                    <div class="project-description">
                        Looking at the evaluation of knowledge extraction and synthesis frameworks in general, and RAG in particular, using GRACE – a multi-agent RAG framework – for research and deployment. We are looking at systematic evaluation of the novel features of GRACE and comparison to similar systems, and at how evaluation at a query level can be performed and played back to the user. Focussing on economic and prudential data.
                    </div>
                </div>

                <div class="project">
                    <div class="project-title">Enhancing BM25-based search</div>
                    <div class="project-highlight">
                        The BM25 method has been around about 50 years now but is still a great way to find information – basically like doing Ctrl+F, but with corrections such as weighting towards rarer words. It's fast, robust and explainable – how can we make it better?
                    </div>
                    <div class="project-description">
                        GRACE uses search techniques which extend BM25. A baseline BM25 (Lucene, with stemming) benefits from low import times and good performance – particularly with rare or domain specific words. This work looks at domain specific query expansion, particularly for providing context for RAG, so evaluation of it overlaps with the work above.
                    </div>
                </div>

                <div class="project">
                    <div class="project-title">Word embeddings as fixed points</div>
                    <div class="project-highlight">
                        Word embeddings represent words as vectors of numbers, where the relative positions of those vectors have meaning. There are now much more sophisticated ways of representing words in context within a sentence, so results won't be state-of-the-art, but it's interesting to look at how representations are formed, and novel ways of doing that.
                    </div>
                    <div class="project-description">
                        Finding word embeddings as fixed points of forward mappings – explicitly, defining simple forward mapping to a word from its neighbours, with negative sampling becoming orthonormalization. Usefulness of this (it needs little data), quality and flavour of the embeddings (word level and on MTEB – some capture semantic features and others achieve the aim of being more like dependency-based ones), and links to the query expansion above. I'm also interested in lightweight, and training free, attention.
                    </div>
                </div>

                <div class="project">
                    <div class="project-title">Expectation Formation in Large Language Models</div>
                    <div class="project-collaborators">with Niki Anesti and Andi Joseph</div>
                    <div class="project-highlight">
                        An LLM is told to behave like a given person (e.g. 30 years old, male, employed, living in the North-East…), and given some basic idea about the inflation in the economy at a given time. How does it think that person behaves, or how does it behave like that person? Does it replicate stylised facts about inflation expectations? Are the effects across demographics correct or interesting?
                    </div>
                    <div class="project-description">
                        Comparing inflation expectations and perceptions elicited from GPT-3.5 with different demographic conditioning, from humans in the IAS survey, and (for perceptions) the ONS out-turn. We use the inflation peak in early 2023 as an out-of-time (for GPT-3.5-0613) and out-of-experience test of the similarities and differences between these three measures.
                    </div>
                </div>

                <div class="project">
                    <div class="project-title">Sentiment Decomposition</div>
                    <div class="project-collaborators">with Iulia Bucur – Bank Underground – due to be published soon!</div>
                </div>

                <div class="project">
                    <div class="project-title">Understanding Monetary Policy Uncertainty</div>
                    <div class="project-collaborators">with Carlos Canon Salazar, and Wonwoo Bae, Shinpei Nakamura and Chris Wu at Yale</div>
                </div>

                <h3 class="subsection-title">Macroeconomic Modelling</h3>

                <div class="project">
                    <div class="project-title">Efficient estimation of the steady state of heterogeneous agent (HA) models</div>
                    <div class="project-collaborators">with Jamie Lenney</div>
                    <div class="project-description">
                        Standard optimisation routines struggle with these problems due to a costly-to-evaluation function to be optimised, a globally OK-behaved but locally discontinuous optimisation surface, and an increasing dimensionality of targets and domain. We improve in both the number of function evaluations and stability, and demonstrate the methods on smooth and jumpy surfaces.
                    </div>
                </div>

                <div class="project">
                    <div class="project-title">A Hybrid Deep Reinforcement Learning approach to global HA model solutions</div>
                    <div class="project-collaborators">with Jamie Lenney</div>
                    <div class="project-description">
                        A scheme allowing non-linear solutions to complex (e.g. discrete choice) HA models under aggregate shocks.
                    </div>
                </div>

                <div class="project">
                    <div class="project-title">A HANK model for the UK</div>
                    <div class="project-collaborators">with Daniel Albuquerque, Sean Lavender, Jamie Lenney and Alberto Polo</div>
                    <div class="project-description">
                        The design and estimation of a UK-specific HANK model and policy experiments with it.
                    </div>
                </div>
            </section>

            <section class="section">
                <h2 class="section-title">Previous Work</h2>
                <p>Prior to my current role at the Bank of England (2019-), I co-founded Count (count.co, 2016-19), was a PhD student, post-doc and research fellow in the Plasma Physics Group at Imperial College, London (2007-16), and did the Maths Tripos at Trinity College, Cambridge (2003-07).</p>
                <p>See <a href="https://scholar.google.com/citations?hl=en&user=mutaoa0AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> for my publications. <a href="https://www.linkedin.com/in/ed-hill/">LinkedIn</a>.</p>
                <p>I enjoy running, playing the piano and learning languages (not to the point of being very good at them, but I like to compare and contrast – Arabic, Mandarin, Japanese, …)</p>
            </section>
        </div>
    </div>

    <footer class="footer-section">
        <p>All views expressed on this site are my own and do not necessarily reflect those of any entity with which I am affiliated.</p>
    </footer>
</body>
</html>
